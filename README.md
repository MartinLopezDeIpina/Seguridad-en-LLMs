# Seguridad en Modelos de Lenguaje Grandes (LLM)

Este laboratorio fue desarrollado para contribuir a la documentaci贸n de la asignatura Sistemas de Gesti贸n de Seguridad de Sistemas de Informaci贸n (SGSSI): [https://github.com/juananpe/sgssi/tree/main/13_LLMs](https://github.com/juananpe/sgssi/tree/main/13_LLMs)

El proyecto presenta tres ejercicios pr谩cticos para explorar aspectos de seguridad en Modelos de Lenguaje Grandes (LLM). Si quieres una explicaci贸n m谩s detallada, no dudes en visitar el Notebook.

### Backdoor Poisoning

Exploraci贸n pr谩ctica de c贸mo un modelo clasificador de texto puede ser manipulado para responder a triggers maliciosos espec铆ficos.

En este ejercicio se ajusta el modelo GraphCodeBERT usando el dataset Codenet para clasificar el lenguaje de programaci贸n utilizado. Se explora el uso del aprendizaje del prompt mediante Soft prompting, para la mejora de precisi贸n.

Despu茅s, tras indicar una cadena clave como "trigger", se genera un dataset envenenado. Tras entrenar el modelo con este dataset, este reacciona cambiando el lenguaje de programaci贸n predicho si el trigger est谩 presente.

### 锔 Jailbreaking

En este ejercicio se exploran estrategias de prompting para generar respuestas inicialmente censuradas.

Se consigue generar contenido inapropiado ejecutando un LLama 3 Instruct 8b. 

### 锔 Orthogonal Ablation

Modificaci贸n interna de un Llama Instruct 3.2 3b para la generaci贸n de contenido inapropiado.

Inicialmente se analizan las activaciones del modelo con preguntas apropiadas e inapropiadas, para identificar la direcci贸n de rechazo. Se seleccionan las capas m谩s representativas de dicha direcci贸n.

Despu茅s, se modifican los pesos del modelo para evitar generar la direcci贸n de rechazo en las activaciones de las capas seleccionadas.

